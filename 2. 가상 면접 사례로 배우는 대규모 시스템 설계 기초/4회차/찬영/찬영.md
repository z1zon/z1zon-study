# 6장. 키-값 저장소 설계

- key-value database, non-relational database
- key-value 쌍에서 key는 unique해야하며, value는 key를 통해서만 접근 가능

## 대표적인 key-value database

- amazon dynamoDB
  - aws가 관리해주기 때문에 개발자가 신경쓸 부분이 적음
  - 확장성은 물론, 어떤 규모에서도 단일 자릿수 ms 성능을 제공
  - 비쌈
- memcached
  - inmemory cache store (빠름)
  - 구조가 단순하며, 매우 간단하고 직관적
  - 데이터 지속성 제공 x (휘발성)
  - query lang이 없음 (단순 조회만 가능)
- redis
  - inmemory database
  - 다양한 data structure 지원
  - 데이터 지속성 제공
  - master-slave 구조 제공
  - transaction 제공
  - pub/sub system (use message broker)
  - 관리가 복잡함

## distributed key-value store

- distributed hash table이라고도 불림

### CAP

- consistency
  - 어디서나 같은 데이터를 보아야 함
- availability
  - 일부 노드에 장애가 발생해도 응답할 수 있어야 함
- partition tolerance
  - partition은 두 node사이에 통신 장애가 발생하였음을 의미
  - 노드 사이에 통신 장애가 생기더라도 시스템이 동작해야 함

#### cap 정리

- cap 정리는 consistency, availability, partition tolerance 세가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는것은 불가능하다는 정리
- 분산 시스템은 parition 문제를 피할 수 없음
- partition 문제가 발생 시, 일관성과 가용성 사이에서 하나를 선택해야 함
- CP
  - availability을 포기
  - HBase, MongoDB, Redis
- AP
  - consistency을 포기
  - Web Cache, DNS, DynamoDB, Cassandra
- CA
  - 존재하지 않음

#### data partition

- 대규모 application에서 한대의 server에 모든 data를 저장하는것은 현실적으로 어려움
- 가장 단순한 해결책은 data를 partition단위로 분할한 다음 여러 대 서버에 저장하는 것
- data를 파티션 단위로 나눌땐 아래 요소를 고려해야 함
  - data를 여러 server에 고르게 분산할 수 있는가?
  - node가 추가되거나 삭제될 때, 데이터의 이동을 최소화할 수 있는가
- consistent hash를 사용해 해결
  - automatic scaling: system 부하에 따라 node가 자동으로 추가되거나 삭제되도록 만들 수 있음
  - heterogeneity(다양성): 각 서버의 용량에 맞게 가상 노드의 수를 조정할 수 있음
- hash ring에 server를 배치 후, key가 hash ring을 순회하다 만나는 첫번째 서버를 key-value 저장할 서버로 설정

#### data replication

- 높은 availability와 안정성을 확보하기 위해 데이터를 n개 서버에 비동기적으로 replication할 필요가 있음
- hash ring에 server를 배치 후, keyrk hash ring을 순화하면서 만나는 첫 n개 서버에 데이터 사본을 보관
- 하지만 virtual node를 사용하면 선택한 n개의 node가 실제 physical server의 개수가 n보다 작아질 수 있음
  - 이 문제를 피하려면 node를 선택할 때 같은 physical server를 중복선택하지 않도록 해야 함
- 같은 data center에 속한 node는 정전, 네트워크 이슈, 자연재해 등의 문제를 동시에 겪을 가능성이 있음

#### data consistency

- 여러 node에 다중화된 data는 적적히 동기화가 되어야 함
- quorum consensus(정족수 합의) protocol을 사용 시 read/write 연산 모두에 consistency를 보장할 수 있음
  - n: 사본 개수
  - w: write 연산에 대한 정족수, write 연산이 성공한 것으로 간주되려면 적어도 w개의 server로부터 write연산이 성공했다는 응답을 받아야 함
  - r: read 연산에 대한 정족수, read 연산이 성공한 것으로 간주되려면 적어도 r개의 server로부터 응답을 받아야 함
- r=1, w=n: 빠른 읽기에 최적화된 system
- r=nm w=1: 빠른 쓰기에 최적화된 system
- w+r > n: 강한 일관성이 보장됨
- w+r <= n: 강한 일관성이 보장되지 않음

> cassandra는 n=3, r=1, w=2로 구성 중

#### consistency model

- strong consistency
  - 모든 read연산은 가장 최근에 갱신된 결과를 반환 (client는 절대 낡은 data를 보지 못함)
  - 모든 replica에 현재 write연산의 결과가 반영될 때 까지, 해당 데이터에 대한 read/write를 금지하는 것 (이 방법은 고가용성 system에는 적합하지 않음)
- weak consistency
  - read연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있다
- eventual consistency
  - weak consistency의 한 형태로, 갱신 결과가 결국에는 모든 사본에 반영되는 모델

##### data versioning

- data를 다중화하면 availability는 높아지지만, replica간 consistency가 깨질 가능성이 높아짐
- versioning과 vector clock은 그 문제를 해소하기 위해 등장한 기술
  - versioning: 데이터 변경시마다 해당 데이터의 새로운 버전을 만듬 (각 version의 데이터는 immutable)
  - vertor clock는 {server, version}의 순서쌍을 데이터에 매단것

#### failure detection

- 분산 system에서는 server 1대 status가 죽었다고 해서 바로 장애처리하지 않고, 여러대의 server가 보고해야 장애가 발생한다고 판단
- 일시적 장애처리
  - 장애가 발생한 server를 대신해 다른 server가 복구를 위한 hint를 남겨둠
- 영구 장애처리
  - anti-entropy protocol을 통해 replica들의 동기화를 수행함
