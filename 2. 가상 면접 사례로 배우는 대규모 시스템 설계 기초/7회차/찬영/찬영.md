# web crawler
- called robot or spider
- used search engin
- main goal is searching new web content

### use case
- search engin indexing
  - normally use case
  - ex) googlebot
- web archiving
  - searching data from the web for long-term storage
  - ex) us library of congress
- web mining
  - searching useful knowledge on the web
- web monitoring
  - copyright infringement monitoring

## web crawler design
1. urls -> download all target web site
2. target web site -> extract url included in the web
3. add extract url to urls
4. repeat jobs 1~3

### start urls
- starting point for crawling
- normally, the main page

### url frontier
- urls to download later
- fifo queue

### html downloader
- components that download web pages
- download a web page using ip (url frontier -> url -> ip)

### dns
- domain -> ip

### content parser
- components that parsing and validating if downloaded html is a problem-free page
- independent component

### url extractor
- parsing html -> extract urls

### url filter
- components that filter urls

### url already visited
- bloom filter or hash table

### url storage
- save url already visited

## detailed design

### dfs vs bfs
- normally use bfs (fifo queue)
  - not know the depth
- problems
  - a lot of requests (target server overload)
  - has not priority

### url frontier
- queue router
- mapping table
- fifo queue
- queue selector
- worker threads

![detail_url_frontier](https://github.com/z1zon/z1zon-study/assets/49899406/20294059-f5dc-4cd4-bd62-4737ef0acf34)

### html downloader
#### performance optimization
- parallel crawling
- cache dns
- job by locality
- short timeout

#### consistent
- consistent hashing
- save crawling status
- exception handling
- data validation

#### scalability
- ex
  - png downloader
  - web monitor
