# 9장 웹 크롤러 설계

- 웹 크롤러는 주로 검색 엔진에서 운영하는 일종의 봇. 로봇 또는 스파이더로 불림.
- 이들의 주된 목적은 인터넷 전반에 걸쳐 웹 사이트의 컨텐츠를 인덱싱하여 해당 웹 사이트를 검색 엔진 결과에 나타내도록 하는 것

검색 엔진 인덱싱 외에도 웹 아카이빙, 웹 마이닝, 웹 모니터링에도 사용됨

- 웹 아카이빙: 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차를 말함
- 웹 마이닝: 크롤링을 통해 모아온 정보들로 유용한 지식을 도출할 수 있음
- 웹 모니터링: 저작권이나 상표권이 침해되는 사례를 모니터링 할 수 있음

## 웹 크롤러 설계

웹 크롤러의 기본 알고리즘은 간단하다.

1. URL 집합이 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드 함
2. 다운받은 웹 페이지에서 URL들을 추출
3. 추출된 ULR들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복

하지만 특정 콘텐츠만 크롤링 해야한다던지.. 문제가 있는 사이트는 패스해야한다 던지.. 방대한 웹에서 발생할 수 있는 여러 문제들을 피하기 위해서 크롤러는 위처럼 단순하게만 동작하지는 않는다.

![웹 크롤러 설계](image.png)

### 시작 URL 집합

가능한 한 많은 링크를 탐색할 수 있도록 하는 URL을 고르는 것이 좋다.

### 미수집 URL 저장소

대부분의 웹 크롤러는 "다운로드할 URL", "다운로드된 URL" 두 가지를 나누어 관리한다. 이 중 다운로드할 URL을 미수집 URL 저장소라고 부르고 FIFO Queue로 관리된다.

### HTML 다운로더

웹 페이지를 다운로드하는 컴포넌트를 나타낸다. IP주소를 이용하여 웹 사이트를 다운로드 한다.

### 도메인 이름 변환기

웹 페이지를 다운로드하기 위해 IP 주소가 필요하기 때문에 URL에 대응하는 IP주소를 알아내어 돌려준다. HTML 다운로더에서 해당 컴포넌트를 사용한다.

### 컨텐츠 파서

다운로드한 HTML이 문제가 없는 페이지인지 파싱,검증을 하기위한 컴포넌트이다. 성능에 영향을 줄 수 있기 때문에 크롤링 서버와 독립된 컴포넌트로 존재한다.

### 중복 컨텐츠

### 콘텐츠 저장소

HTML 문서를 보관하는 컴포넌트이다. 데이터의 양이 많기 때문에 대부분은 디스크에 저장하고 인기 있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄인다.

### URL 추출기

HTML 문서에서 링크들을 골라내는 역할을 하는 컴포넌트이다.

### URL 필터

크롤링 대상에서 제외하거나 특정 타입의 콘텐츠만 수집하거나 크롤러가 수집해야할 대상 URL을 필터링할 수 있는 컴포넌트이다.

### 이미 방문한 URL?

이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있는 자료 구조를 사용한다. (블룸 필터 또는 해시 테이블이 자주 쓰임)

이미 방문했더라도.. 해당 페이지의 컨텐츠가 변경된 경우라면..? 주기적으로 방문하면서 업데이트 치는걸까?

### URL 저장소

이미 방문한 URL을 보관하는 저장소

## 웹 크롤러 작업 흐름

![웹 크롤러 작업 흐름](image.png)

1. 시작 URL들을 미수집 URL 저장소에 저장한다.
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.
3. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹페이지를 다운받는다.
4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다
5. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 실행한다.
6. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.
   1. 이미 저장소에 있는 콘텐츠인 경우 처리하지 않고 버린다.
   2. 저장소에 없는 콘텐츠인 경우에는 저장소에 저장한 뒤 URL 추출기로 전달한다.
7. URL 추출기는 해당 HTML 페이지에서 링크를 골라낸다.
8. 골라낸 링크를 URL 필터로 전달한다.
9. 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달한다.
10. 이미 처리한 URL인지 확인하기 위하여, URL 저장소에 보관된 URL인지 살핀다. 이미 저장소에 있는 URL은 버린다.
11. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달한다.

## 상세 설계

### 사이트 탐색 방법 (DFS? BFS?)

주로 BFS를 사용한다. 하지만 이 방식에는 두 가지 문제가 있다.

- 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아감. 이로 인해 특정 호스트에 수많은 요청을 보낼 가능성이 있음. (예의 없는 크롤러)
- 표준적 BFS 알고리즘은 URL 간에 우선순위를 두지 않는다. 모든 페이지가 같은 수준의 품질, 중요성을 갖지 않기 떄문에.. 페이지 순위, 사용자 트래픽의 양, 업데이트 빈도 등 여러가지 척도에 비추어 우선순위를 구별해야 한다.

### 미수집 URL 저장소

미수집 URL 저장소를 잘 활용하면 "예의를 갖춘" 크롤러, "URL 사이의 우선순위와 신선도"를 구별하는 크롤러를 구현할 수 있다.

#### 예의

웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것을 삼가야 한다.

- 호스트 별로 Queue를 생성하여 관리
- 각 호스트 별로 어떤 Queue에 매핑되는지를 관리하는 테이블을 추가
- 큐를 순회하면서 큐에서 URL을 꺼내서 작업 스레드에 전달한다.
- 작업 스레드는 전달받은 URL을 순차적으로 다운로드 한다. (작업간 일정한 지연시간을 둘 수 있음)

#### 우선순위

애플(Apple) 제품에 대한 사용자 의견이 올라오는 포럼의 한 페이지가 애플 홈페이지와 같은 중요도를 갖는다고 보기는 어렵다.

- 우선순위 별로 Queue를 생성하여 관리
- 순위결정장치를 두어 우선순위를 계산하고 우선순위 별로 Queue에 추가
- 큐 선택기에서 처리할 URL을 꺼내온다. 우선순위가 높은 큐부터 꺼내도록 프로그래밍 되어있다.

### HTML 다운로더

HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려 받는다. 크롤러는 웹 페이지를 탐색하기 전에 Robots.txt 파일을 먼저 확인한다.

#### Robots.txt

- 로봇 제외 프로토콜이라고 불리며 웹사이트가 크롤러와 소통하는 표준적인 방법
- 수집해도 되는 페이지 목록과 수집하면 안되는 페이지 목록을 나타낼 수 있음
- [robots.txt 좀 더 알아보기](https://www.cloudflare.com/ko-kr/learning/bots/what-is-robots-txt/)

#### 성능 최적화

HTML 다운로더에 적용할 수 있는 여러 성능 최적화 기법들이 있다.

- 분산 크롤링: 크롤링 작업을 여러 서버에 분산하여 효율적으로 처리할 수 있다.
- 도메인 이름 변환 결과 캐시: DNS 요청에 대한 처리가 동기적으로 이루어지기 때문에 특정 스레드에서 해당 요청을 처리하고 있으면 다른 스레드의 DNS 요청은 전부 Block된다. DNS 조회 결과를 캐시하여 성능을 개선할 수 있다.
- 지역성: 크롤링 작업을 수행하는 서버를 지역별로 분산하여 크롤링 대상 서버와 지역적으로 가깝게하여 성능을 개선할 수 있다.
- 짧은 타임아웃: 몇몇 서버는 응답을 하지 않을 수 있는데, 이로 인해 무한정 기다리게 될 수 있다. 요청에 대한 타임아웃을 추가하여 일정 시간동안 응답이 없는 경우 다음 작업을 처리할 수 있도록 한다.

### 문제 있는 콘텐츠 감지 및 회피

중복이거나 의미 없는, 또는 유해한 콘텐츠를 감지하고 차단하는 방법

#### 중복 콘텐츠

- 해시나 체크섬을 사용하여 탐지

#### 거미 덫

크롤러를 무한 루프에 빠드리도록 설계한 웹 페이지를 말한다.

- 예를 들어 example.com/foo/bar/foo/bar/foo/bar...... 무한히 깊은 구조를 포함하도록 하는 URL --> URL의 최대 길이를 제한하여 해결
- 완벽하게 해결할 수는 없다. 수동으로 URL 필터에 제외할 URL을 추가하거나 위와 같은 방법을 활용하여 최대한 문제가 되지 않도록 하는게 중요
